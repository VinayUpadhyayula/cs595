Enrolled in a small course on udemy-
NLP - Natural Language Processing with Python by Jose Portilla

Will be adding important notes here....

NLP aims at understanding human language in human readable language format itself and then convert it onto machine understandable form and performs
various actions based on the algortihm chosen and requitement.

Spacy and NLTK are two main libraries used for NLP.

Spacy: NLP library for implementing algorithms. It is faster and efficient but user doesnot have a choive to choose different algorithms that he need.

NLTK: Natural Language Tool Kit is an open source NLP library that has many functionalities and choice for the user to choose his best fit of algorithms but is less efficient comparitive to Spacy.

In the context of sentiment analysis, spacy doesn't include models which is typically easier to perform using NLTK.

I am trying to first understand spacy and then explore NLTK:

Text data is highly unstructured and can be in multiple languages.

nlp() function from spacey automatically takes raw text and performs a series of operations to tag, parse and describe the text data.

Spacy installation for macOS:
 `conda install -c conda-forge spacy`
  `python -m spacy download en`

# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_)

Output:
Tesla PROPN nsubj
is VERB aux
looking VERB ROOT
at ADP prep
buying VERB pcomp
U.S. PROPN compound
startup NOUN dobj
for ADP prep
$ SYM quantmod
6 NUM compound
million NUM pobj

This looks not so user-friendly, but right away we see some interesting things happened:
1. Tesla is recognized to be a Proper Noun, not just a word at the start of a sentence
2. U.S. is kept together as one entity (we call this a 'token')

token.text-> retrieves text from the tokens generated
token.pos->parts of speech recognition i.e, if its a noun, VERB, num etc
token.dep->syntactic dependency(should learn what it is in the course...)

___
# Pipeline
When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.

nlp.pipeline
[('tagger', <spacy.pipeline.Tagger at 0x237cb1e8f98>),
 ('parser', <spacy.pipeline.DependencyParser at 0x237cb2852b0>),
 ('ner', <spacy.pipeline.EntityRecognizer at 0x237cb285360>)]

 #Tokenization

 Tokens are the basic building blocks of the document object. Spacy is very smart enough when it divides tokens. Tokens are divided into 
1)Prefix: which start with $,(," etc
2)Suffix: which end with km, ), ., !," etc
3)Infix: -, --, ..., / etc
4)Exception: let's U.S.

But punctuation is present as part of email address or website is kept as a same token.
doc2 = nlp(u"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!")

for t in doc2:
    print(t)

We
're
here
to
help
!
Send
snail
-
mail
,
email
support@oursite.com
or
visit
us
at
http://www.oursite.com
!

The en_core_web_sm library has almost 57852 tokens.

It is worthy to note that re-assignment of document tokens is not possib;e.
doc[0] = "title" gives an error.
TypeError: 'spacy.tokens.doc.Doc' object does not support item assignment.

# Named Entities
Going a step beyond tokens, *named entities* add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the `ents` property of a `Doc` object.

doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')

for token in doc8:
    print(token.text, end=' | ')

print('\n----')

for ent in doc8.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))

Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | 
----
Apple - ORG - Companies, agencies, institutions, etc.
Hong Kong - GPE - Countries, cities, states
$6 million - MONEY - Monetary values, including unit

Named Entity Recognition (NER) is an important machine learning tool applied to Natural Language Processing.

---
# Noun Chunks
Similar to `Doc.ents`, `Doc.noun_chunks` are another object property. *Noun chunks* are "base noun phrases" â€“ flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun.

spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away.

doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')
displacy.render(doc, style='ent', jupyter=True)
