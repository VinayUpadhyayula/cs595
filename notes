Enrolled in a small course on udemy-
NLP - Natural Language Processing with Python by Jose Portilla

Will be adding important notes here....

NLP aims at understanding human language in human readable language format itself and then convert it onto machine understandable form and performs
various actions based on the algortihm chosen and requitement.

Spacy and NLTK are two main libraries used for NLP.

Spacy: NLP library for implementing algorithms. It is faster and efficient but user doesnot have a choive to choose different algorithms that he need.

NLTK: Natural Language Tool Kit is an open source NLP library that has many functionalities and choice for the user to choose his best fit of algorithms but is less efficient comparitive to Spacy.

In the context of sentiment analysis, spacy doesn't include models which is typically easier to perform using NLTK.

I am trying to first understand spacy and then explore NLTK:

Text data is highly unstructured and can be in multiple languages.

nlp() function from spacey automatically takes raw text and performs a series of operations to tag, parse and describe the text data.

Spacy installation for macOS:
 `conda install -c conda-forge spacy`
  `python -m spacy download en`

# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_)

Output:
Tesla PROPN nsubj
is VERB aux
looking VERB ROOT
at ADP prep
buying VERB pcomp
U.S. PROPN compound
startup NOUN dobj
for ADP prep
$ SYM quantmod
6 NUM compound
million NUM pobj

This looks not so user-friendly, but right away we see some interesting things happened:
1. Tesla is recognized to be a Proper Noun, not just a word at the start of a sentence
2. U.S. is kept together as one entity (we call this a 'token')

token.text-> retrieves text from the tokens generated
token.pos->parts of speech recognition i.e, if its a noun, VERB, num etc
token.dep->syntactic dependency(should learn what it is in the course...)

___
# Pipeline
When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.

nlp.pipeline
[('tagger', <spacy.pipeline.Tagger at 0x237cb1e8f98>),
 ('parser', <spacy.pipeline.DependencyParser at 0x237cb2852b0>),
 ('ner', <spacy.pipeline.EntityRecognizer at 0x237cb285360>)]

 #Tokenization

 Tokens are the basic building blocks of the document object. Spacy is very smart enough when it divides tokens. Tokens are divided into 
1)Prefix: which start with $,(," etc
2)Suffix: which end with km, ), ., !," etc
3)Infix: -, --, ..., / etc
4)Exception: let's U.S.

But punctuation is present as part of email address or website is kept as a same token.
doc2 = nlp(u"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!")

for t in doc2:
    print(t)

We
're
here
to
help
!
Send
snail
-
mail
,
email
support@oursite.com
or
visit
us
at
http://www.oursite.com
!

The en_core_web_sm library has almost 57852 tokens.

It is worthy to note that re-assignment of document tokens is not possib;e.
doc[0] = "title" gives an error.
TypeError: 'spacy.tokens.doc.Doc' object does not support item assignment.

# Named Entities
Going a step beyond tokens, *named entities* add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the `ents` property of a `Doc` object.

doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')

for token in doc8:
    print(token.text, end=' | ')

print('\n----')

for ent in doc8.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))

Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | 
----
Apple - ORG - Companies, agencies, institutions, etc.
Hong Kong - GPE - Countries, cities, states
$6 million - MONEY - Monetary values, including unit

Named Entity Recognition (NER) is an important machine learning tool applied to Natural Language Processing.

---
# Noun Chunks
Similar to `Doc.ents`, `Doc.noun_chunks` are another object property. *Noun chunks* are "base noun phrases" â€“ flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun.

spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away.

doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')
displacy.render(doc, style='ent', jupyter=True)

------
# Lemmatization

In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence.

doc3 = nlp(u"I am meeting him tomorrow at the meeting.")
show_lemmas(doc3)

I            PRON   561228191312463089     -PRON-
am           VERB   10382539506755952630   be
meeting      VERB   6880656908171229526    meet
him          PRON   561228191312463089     -PRON-
tomorrow     NOUN   3573583789758258062    tomorrow
at           ADP    11667289587015813222   at
the          DET    7425985699627899538    the
meeting      NOUN   14798207169164081740   meeting
.            PUNCT  12646065887601541794   .

It is to note that Note that lemmatization does not reduce words to their most basic synonym - that is, enormous doesn't become big and automobile doesn't become car.

---
# Stop Words
Words like "a" and "the" appear so frequently that they don't require tagging as thoroughly as nouns, verbs and modifiers. We call these stop words, and they can be filtered from the text to be processed. spaCy holds a built-in list of some 305 English stop words.

## To add a stop word
There may be times when you wish to add a stop word to the default set. Perhaps you decide that `'btw'` (common shorthand for "by the way") should be considered a stop word.

# Add the word to the set of stop words. Use lowercase!
nlp.Defaults.stop_words.add('btw')

# Set the stop_word tag on the lexeme
nlp.vocab['btw'].is_stop = True

Alternatively to remove:

# Remove the word from the set of stop words
nlp.Defaults.stop_words.remove('beyond')

# Remove the stop_word tag from the lexeme
nlp.vocab['beyond'].is_stop = False