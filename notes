Enrolled in a small course on udemy-
NLP - Natural Language Processing with Python by Jose Portilla

Will be adding important notes here....

NLP aims at understanding human language in human readable language format itself and then convert it onto machine understandable form and performs
various actions based on the algortihm chosen and requitement.

Spacy and NLTK are two main libraries used for NLP.

Spacy: NLP library for implementing algorithms. It is faster and efficient but user doesnot have a choive to choose different algorithms that he need.

NLTK: Natural Language Tool Kit is an open source NLP library that has many functionalities and choice for the user to choose his best fit of algorithms but is less efficient comparitive to Spacy.

In the context of sentiment analysis, spacy doesn't include models which is typically easier to perform using NLTK.

I am trying to first understand spacy and then explore NLTK:

Text data is highly unstructured and can be in multiple languages.

nlp() function from spacey automatically takes raw text and performs a series of operations to tag, parse and describe the text data.

Spacy installation for macOS:
 `conda install -c conda-forge spacy`
  `python -m spacy download en`

# Import spaCy and load the language library
import spacy
nlp = spacy.load('en_core_web_sm')

# Create a Doc object
doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')

# Print each token separately
for token in doc:
    print(token.text, token.pos_, token.dep_)

Output:
Tesla PROPN nsubj
is VERB aux
looking VERB ROOT
at ADP prep
buying VERB pcomp
U.S. PROPN compound
startup NOUN dobj
for ADP prep
$ SYM quantmod
6 NUM compound
million NUM pobj

This looks not so user-friendly, but right away we see some interesting things happened:
1. Tesla is recognized to be a Proper Noun, not just a word at the start of a sentence
2. U.S. is kept together as one entity (we call this a 'token')

token.text-> retrieves text from the tokens generated
token.pos->parts of speech recognition i.e, if its a noun, VERB, num etc
token.dep->syntactic dependency(should learn what it is in the course...)

___
# Pipeline
When we run `nlp`, our text enters a *processing pipeline* that first breaks down the text and then performs a series of operations to tag, parse and describe the data.

nlp.pipeline
[('tagger', <spacy.pipeline.Tagger at 0x237cb1e8f98>),
 ('parser', <spacy.pipeline.DependencyParser at 0x237cb2852b0>),
 ('ner', <spacy.pipeline.EntityRecognizer at 0x237cb285360>)]

 #Tokenization

 Tokens are the basic building blocks of the document object. Spacy is very smart enough when it divides tokens. Tokens are divided into 
1)Prefix: which start with $,(," etc
2)Suffix: which end with km, ), ., !," etc
3)Infix: -, --, ..., / etc
4)Exception: let's U.S.

But punctuation is present as part of email address or website is kept as a same token.
doc2 = nlp(u"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!")

for t in doc2:
    print(t)

We
're
here
to
help
!
Send
snail
-
mail
,
email
support@oursite.com
or
visit
us
at
http://www.oursite.com
!

The en_core_web_sm library has almost 57852 tokens.

It is worthy to note that re-assignment of document tokens is not possib;e.
doc[0] = "title" gives an error.
TypeError: 'spacy.tokens.doc.Doc' object does not support item assignment.

# Named Entities
Going a step beyond tokens, *named entities* add another layer of context. The language model recognizes that certain words are organizational names while others are locations, and still other combinations relate to money, dates, etc. Named entities are accessible through the `ents` property of a `Doc` object.

doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')

for token in doc8:
    print(token.text, end=' | ')

print('\n----')

for ent in doc8.ents:
    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))

Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | 
----
Apple - ORG - Companies, agencies, institutions, etc.
Hong Kong - GPE - Countries, cities, states
$6 million - MONEY - Monetary values, including unit

Named Entity Recognition (NER) is an important machine learning tool applied to Natural Language Processing.

---
# Noun Chunks
Similar to `Doc.ents`, `Doc.noun_chunks` are another object property. *Noun chunks* are "base noun phrases" â€“ flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun.

spaCy includes a built-in visualization tool called **displaCy**. displaCy is able to detect whether you're working in a Jupyter notebook, and will return markup that can be rendered in a cell right away.

doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')
displacy.render(doc, style='ent', jupyter=True)

------
# Lemmatization

In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'. Further, the lemma of 'meeting' might be 'meet' or 'meeting' depending on its use in a sentence.

doc3 = nlp(u"I am meeting him tomorrow at the meeting.")
show_lemmas(doc3)

I            PRON   561228191312463089     -PRON-
am           VERB   10382539506755952630   be
meeting      VERB   6880656908171229526    meet
him          PRON   561228191312463089     -PRON-
tomorrow     NOUN   3573583789758258062    tomorrow
at           ADP    11667289587015813222   at
the          DET    7425985699627899538    the
meeting      NOUN   14798207169164081740   meeting
.            PUNCT  12646065887601541794   .

It is to note that Note that lemmatization does not reduce words to their most basic synonym - that is, enormous doesn't become big and automobile doesn't become car.

---
# Stop Words
Words like "a" and "the" appear so frequently that they don't require tagging as thoroughly as nouns, verbs and modifiers. We call these stop words, and they can be filtered from the text to be processed. spaCy holds a built-in list of some 305 English stop words.

## To add a stop word
There may be times when you wish to add a stop word to the default set. Perhaps you decide that `'btw'` (common shorthand for "by the way") should be considered a stop word.

# Add the word to the set of stop words. Use lowercase!
nlp.Defaults.stop_words.add('btw')

# Set the stop_word tag on the lexeme
nlp.vocab['btw'].is_stop = True

Alternatively to remove:

# Remove the word from the set of stop words
nlp.Defaults.stop_words.remove('beyond')

# Remove the stop_word tag from the lexeme
nlp.vocab['beyond'].is_stop = False

---------
# Vocabulary and Matching
I have learnt how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.

In this section I learnt to identify and label specific phrases that match patterns we can define ourselves.

## Rule-based Matching
spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher.

# Import the Matcher library
from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)

Here Matcher is the inbuilt function which we point to our nlp vocab object. We can define our own patterns to the Matcher to identify

pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]
pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]

matcher.add('SolarPower', None, pattern1, pattern2, pattern3)

* `pattern1` looks for a single token whose lowercase text reads 'solarpower'
* `pattern2` looks for two adjacent tokens that read 'solar' and 'power' in that order
* `pattern3` looks for three adjacent tokens, with a middle token that can be any punctuation.

It is worth rememebering that single spaces are not tokenized and hence don't count as punctuation.

Applying Matcher to a doc object
---------------------------------
doc = nlp(u'The Solar Power industry continues to grow as demand \
for solarpower increases. Solar-power cars are gaining popularity.')

found_matches = matcher(doc)
for match_id, start, end in found_matches:
    string_id = nlp.vocab.strings[match_id]  # get string representation
    span = doc[start:end]                    # get the matched span
    print(match_id, string_id, start, end, span.text)

8656102463236116519 SolarPower 1 3 Solar Power
8656102463236116519 SolarPower 10 11 solarpower
8656102463236116519 SolarPower 13 16 Solar-power

We can also pass an option "OP":'x' as argument in the pattern which can be used to match patterns optionally.

pattern1 = [{'LOWER': 'solarpower'}]
pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]

# Remove the old patterns to avoid duplication:
matcher.remove('SolarPower')

# Add the new set of patterns to the 'SolarPower' matcher:
matcher.add('SolarPower', None, pattern1, pattern2)

## PhraseMatcher
Above we used token patterns to perform rule-based matching. An alternative - and often more efficient - method is to match on terminology lists. In this case we use PhraseMatcher to create a Doc object from a list of phrases, and pass that into `matcher` instead.

# Import the PhraseMatcher library
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)

USing this matcher we can load a document from any directory and perform matching based on the pattern in the entire document, play with the slicing the matches etc.(like matching words around the pattern, match the entire line with the pattern)

# Parts of Speech Tagging

Processing raw text intelligently is difficult: most words are rare, and it's common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages. 

Defining nlp and doc from nlp() as above, 
print(doc[4].text, doc[4].pos_, doc[4].tag_, spacy.explain(doc[4].tag_))
jumped VERB VBD verb, past tense

* To view the coarse POS tag use `token.pos_`
* To view the fine-grained tag use `token.tag_`
* To view the description of either type of tag we can use `spacy.explain(tag)`

# Frequency of POS items

`POS_counts` returns a dictionary, from which we can obtain a list of keys with `POS_counts.items()`.By sorting the list we have access to the tag and its count, in order.

for k,v in sorted(POS_counts.items()):
    print(f'{k}. {doc.vocab[k].text:{5}}: {v}')

83. ADJ  : 3
84. ADP  : 1
89. DET  : 2
91. NOUN : 3
93. PART : 1
96. PUNCT: 1
99. VERB : 1

# Named Entity Recognition (NER)
spaCy has an **'ner'** pipeline component that identifies token spans fitting a predetermined set of named entities. These are available as the `ents` property of a `Doc` object.

import spacy
nlp = spacy.load('en_core_web_sm')

def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))
    else:
        print('No named entities found.')
doc = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')

show_ents(doc)

Output:

Washington, DC - GPE - Countries, cities, states
next May - DATE - Absolute or relative dates or periods
the Washington Monument - ORG - Companies, agencies, institutions, etc.

doc = nlp(u'Can I please borrow 500 dollars from you to buy some Microsoft stock?')

for ent in doc.ents:
    print(ent.text, ent.start, ent.end, ent.start_char, ent.end_char, ent.label_)

Output:

500 dollars 4 6 20 31 MONEY
Microsoft 11 12 53 62 ORG

## Adding a Named Entity to a Span
Normally we would have spaCy build a library of named entities by training it on several samples of text.

from spacy.tokens import Span

# Get the hash value of the ORG entity label
ORG = doc.vocab.strings[u'ORG']  

# Create a Span for the new entity
new_ent = Span(doc, 0, 1, label=ORG)

# Add the entity to the existing Doc object
doc.ents = list(doc.ents) + [new_ent]

# Sentence segmentation

doc = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')

for sent in doc.sents:
    print(sent)

This is the first sentence.
This is another sentence.
This is the last sentence.

print(doc.sents[1])

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
 in ()
----> 1 print(doc.sents[1])

TypeError: 'generator' object is not subscriptable

`doc.sents` is a generator. That is, a Doc is not segmented until `doc.sents` is called. This means that, where we could print the second Doc token with `print(doc[1])`, we can't call the "second Doc sentence" with `print(doc.sents[1])`:

## Adding Rules
spaCy's built-in `sentencizer` relies on the dependency parse and end-of-sentence punctuation to determine segmentation rules. We can add rules of our own, but they have to be added before the creation of the Doc object, as that is where the parsing of segment start tokens happens:

# Add a semicolon to our existing segmentation rules. 
That is, whenever the sentencizer encounters a semicolon, the next token should start a new segment.

# SPACY'S DEFAULT BEHAVIOR
doc3 = nlp(u'"Management is doing things right; leadership is doing the right things." -Peter Drucker')

for sent in doc3.sents:
    print(sent)

"Management is doing things right; leadership is doing the right things."
-Peter Drucker

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ';':
            doc[token.i+1].is_sent_start = True
    return doc

nlp.add_pipe(set_custom_boundaries, before='parser')

nlp.pipe_names

['tagger', 'set_custom_boundaries', 'parser', 'ner']

doc4 = nlp(u'"Management is doing things right; leadership is doing the right things." -Peter Drucker')

for sent in doc4.sents:
    print(sent)

"Management is doing things right;
leadership is doing the right things."
-Peter Drucker

This only applies to the current doc object but not to the entire library , we can achieve that by changing the spacy rule itself by manipulating the token start and end.

# Text Classification
     In machine learning supervised learning is a type of paradigm that works on labelled data and predict based on historial ouput.

On a higher note, the stages involved in a machine learning process are :

1)Data Acquistion - from various online resources or real- time data.
2)Data Cleaning - a pre processing step like vectorization.
3)Train /Test SPlit of input data - mostly 70:30 split
4)Model training - train the model using trained data and fit/predict using the trained model on the 30% data
5)Model Deployment

Each machine learning model predicts almost same output in case of say classification(detecting whether a mail is a HAM or a Spam). So we need various performance metrics to evaluate the accuracy/correctness of the model.

Vectorization : It a process of converting human readable raw text into a vector(matrix) so that it is understandable by the model.

In real world scenario it is not enough to have a single metric as it may not encompass all the scenaroios needed to evaluate the model. So we have many metrics such as:

1) Accuracy = no of correct predictions/ total no of predictions
-- It is useful when we have balanced featured data (ie., 50% HAM, 50% SPAM data in the fetaure set)
--Not very useful when we have un - balanced data (ie, 99%SPAM, 1% HAM data in the feature set)

2)Precision = no of True Positive / no of True Positive + no of False positive
 - It is the ability to identify only relevant data points i.e., proportion of what actually is relevant

3)Recall =  no of True Positive / no of True Positive + no of False negative
- It is the ability to find all the relevant cases within a dataset

4)F1 Score = 2* (precision*recall/precision +recall)
- It is the harmonic mean of recall and precision.

Confusion matrix: It gives the ratio of TP, TN, FP and FN's.

# Scikit Learn 
Scikit learn library is used for all the functions like numpy, pandas, matplotlib and many others for data preprocessing, visualizing data and machine learning model training, prediction and performance metrics.

# Term Frequencey - Inverse Document Frequencey (TF-IDF) Vectorization

Most machine learning models understand numeric data where most of the real world data cannot be and is not and so is out twitter sentiment analysis data. After cleaning the data as part of further pre processing we assign and vectorize the text data into numeric vector using TF - IDF vectorization. Before diving deep into TF - IDF let us undestand first what IDF and TF mean:

IDF(t)  = log(Total no of documents/No of documents t is present in)

TF(t,d) = total number of time term t is present in doc A/ Total no of terms in doc A

Why do we go for TF also but not simpy use IDF?

    One document might contain 1000 words whereas the other might contain only 10 words then it would be unfair and wrong to check for IDF so we take Term frequency which matters to that document

TF - IDF = TF(t) * IDF(t,d)

btw, why use log?? I got a neat and simple answer on Stack Overflow 
https://stackoverflow.com/questions/27067992/why-is-log-used-when-calculating-term-frequency-weight-and-idf-inverse-document

Debasis's answer: 

Here is the intuition: If term frequency for the word 'computer' in doc1 is 10 and in doc2 it's 20, we can say that doc2 is more relevant than doc1 for the word 'computer.

However, if the term frequency of the same word, 'computer', for doc1 is 1 million and doc2 is 2 millions, at this point, there is no much difference in terms of relevancy anymore because they both contain a very high count for term 'computer'.

Just like Debasis's answer, adding log is to dampen the importance of term that has a high frequency, e.g. Using log base 2, the count of 1 million will be reduced to 19.9!

# We do TF-IDF to achieve:

Numerical Representation: Machine learning algorithms typically require numerical input. TF-IDF vectorization converts text data into numerical vectors, allowing machine learning models to process and analyze textual data.

Feature Extraction: TF-IDF vectorization extracts features from text data based on the frequency of words in documents. It assigns higher weights to words that are frequent in a document but rare in other documents, capturing the importance of words in distinguishing documents.

Dimensionality Reduction: TF-IDF vectorization reduces the dimensionality of the text data by representing each document with a sparse vector in a high-dimensional space. This reduces the computational complexity of processing and analyzing text data.

Normalization: TF-IDF normalizes the weights of words based on their frequency and rarity in documents. This helps mitigate the impact of document length and word frequency variations across documents, making the representation more robust.

This basically brings down to one ideology: words that occur more frequently has less relevancy and will be assigned a lower score than unique words which are assigned a higher score.